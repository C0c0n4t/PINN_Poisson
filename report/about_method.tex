\section{Обзор метода}

Идея состоит в том, чтобы решать краевые задачи (определенные на Декартовой прямоугольной системе координат), представляя численное решение с помощью нейронной сети и обучая полученную сеть удовлетворять условиям, требуемым дифференциальным уравнением и граничными условиями.
Это решение включает нейронную сеть в качестве основного элемента аппроксимации, параметры которой (веса и смещения) подстраиваются для минимизации функции ошибки (Loss). Для обучения мы используем методы оптимизации, которые требуют вычисления градиента ошибки по параметрам сети.

\medskip
\medskip

Разберем метод на нашем уравнении Пуассона:
\[
    \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = f(x, y).
\]

Возьмем, например, $\forall x, y \in \Omega=[0, 2] \times [0, 2]$.  Вид граничных условий Дирихле такой:

\[
    u(x, y) = g(x, y), \text{ для } (x, y) \in \partial \Omega.
\]

Чтобы решить эту проблему, мы аппроксимируем решение с помощью нейронной сети:
\[
    NN(x, y) \approx u(x, y).
\]

Если $ NN(x, y) $ является истинным решением, то должно выполняться равенство
\[
    \frac{\partial^2 NN(x, y)}{\partial x^2}  +\frac{\partial^2 NN(x, y)}{\partial y^2} = f(x, y) \quad \forall (x, y) \in \Omega.
\]

Таким образом, мы превращаем это условие в нашу функцию потерь. Это обосновывает выбор функции потерь. Обозначив параметры нейросети (веса и смещения) буквой $\omega$, запишем:

\[
    \begin{split}
        L(\omega) = \frac{1}{n} \sum_{x_i,y_i \in \Omega \backslash \partial \Omega}
        \left( \frac{\partial^2 NN(x_i, y_i)}{\partial x^2} +\frac{\partial^2 NN(x_i,
        y_i)}{\partial y^2} - f(x_i, y_i) \right)^2 \\
        \text{ --- среднеквадратичная функция потерь}
    \end{split}
\]

Выбор $ x_i, y_i $ может быть выполнен различными способами, в нашем случае, времена множество точек представлено в виде сетки прямоугольной равномерной сетки. В любом случае, когда эта функция потерь минимизируется (в нашем случае вычисляем градиенты с помощью стандартного обратного дифференцирования \textrm{tensorflow}), мы имеем, что
\[
    \frac{\partial^2 NN(x_i, y_i)}{\partial x^2}  +\frac{\partial^2 NN(x_i, y_i)}{\partial y^2} \approx f(x_i, y_i),
\]
и, следовательно, $ NN(x, y) $ аппроксимирует решение краевой задачи.

Но нам все еще нужно учесть граниченое условие. Один из простых способов сделать это --- добавить потери на границе в функцию стоимости.

\[
    \begin{split}
         & L(\omega, \lambda) = \frac{1}{n} \sum_{x_i,y_i \in \Omega \backslash \partial \Omega} \left( \frac{\partial^2 NN(x_i, y_i)}{\partial x^2} + \frac{\partial^2 NN(x_i, y_i)}{\partial y^2}  - f(x_i, y_i) \right)^2 \, + \\
         & + \, \frac{\lambda}{n}\sum_{x_i,y_i \in \partial \Omega} (NN(x_i, y_i) - u(x_i, y_i))^2
    \end{split}
\]

где $ \lambda$ --- гиперпараметр, определяющий нейронную сеть $ NN $, аппроксимирующую $ u $. Таким образом, решение задачи снова сводится к нахождению весов, которые минимизируют функцию потерь!

\begin{comment}
(под удаление либо надо что то написать про градиентный спуск)
A. Вычисление градиента
Эффективная минимизация (3) может рассматриваться как
процедуру обучения нейронной сети, где ошибка
соответствующая каждому входному вектору, является значением, которое
должна стать нулевой. В вычислении этого значения ошибки участвуют
не только выход сети (как это происходит при обычном
обучении), но и производные выхода по отношению к
любого из входов. Поэтому при вычислении градиента
ошибки относительно весов сети, нам необходимо вычислить
не только градиент сети, но и градиент
производных сети по отношению к ее входам.
Рассмотрим многослойный перцептрон с входными блоками, одним
скрытым слоем с сигмоидальными блоками и линейным выходным блоком.
Расширение на случай более чем одного скрытого слоя
может быть получено соответствующим образом. Для заданного входного вектора
выход сети имеет вид
где обозначает вес от
входного блока к скрытому блоку, обозначает вес
от скрытого блока к выходу, обозначает смещение
скрытого блока, и сигмоидальная передаточная функция. Легко
легко показать, что
\end{comment}